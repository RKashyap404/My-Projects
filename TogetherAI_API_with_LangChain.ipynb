{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RKashyap404/My-Projects/blob/main/TogetherAI_API_with_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mx-O8RMD4jpt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "import together\n",
        "from langchain.llms.base import LLM\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "class TogetherLLM(LLM):\n",
        "    model: str = \"togethercomputer/llama-2-7b-chat\"\n",
        "    together_api_key: str = os.environ[\"------------\"]\n",
        "    temperature: float = 0.7\n",
        "    max_tokens: int = 512\n",
        "\n",
        "    def __init__(self, model=None, max_tokens=None, temperature=None):\n",
        "        if model:\n",
        "            self.model = model\n",
        "        if max_tokens:\n",
        "            self.max_tokens = max_tokens\n",
        "        if temperature:\n",
        "            self.temperature = temperature\n",
        "\n",
        "    @property\n",
        "    def llm_type(self) -> str:\n",
        "        return \"together\"\n",
        "\n",
        "    def __call__(self, prompt: str, **kwargs) -> str:\n",
        "        try:\n",
        "            logging.info(\"Calling Together endpoint.\")\n",
        "            return self.make_api_call(prompt)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error in TogetherLLM call: {e}\", exc_info=True)\n",
        "            raise\n",
        "\n",
        "    def make_api_call(self, prompt: str) -> str:\n",
        "        together.api_key = self.together_api_key\n",
        "        output = together.Complete.create(\n",
        "            prompt,\n",
        "            model=self.model,\n",
        "            max_tokens=self.max_tokens,\n",
        "            temperature=self.temperature,\n",
        "        )\n",
        "        logging.info(\"API call successful.\")\n",
        "        return output['output']['choices'][0]['text']\n",
        "\n",
        "# Now let's use the class\n",
        "llm = TogetherLLM(\n",
        "    model=\"togethercomputer/llama-2-7b-chat\",\n",
        "    max_tokens=256,\n",
        "    temperature=0.8\n",
        ")\n",
        "\n",
        "prompt_template = \"You are a friendly AI. Answer the following question: {question}\"\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"], template=prompt_template\n",
        ")\n",
        "chat = LLMChain(llm=llm, prompt=prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The integration of TogetherAI with LangChain leverages LangChain's modular architecture to provide a streamlined and efficient interface for large language models (LLMs). The langchain.llms.base.LLM is an abstract base class that simplifies interaction with LLMs by offering a more user-friendly interface, without requiring the user to implement the entire _generate method. This allows for easier integration and customization of various LLMs, such as TogetherAI."
      ],
      "metadata": {
        "id": "oydn_UgY4qhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@property\n",
        "def _llm_type(self) -> str:\n",
        "    \"\"\"Return type of LLM.\"\"\"\n",
        "    return \"together\"\n"
      ],
      "metadata": {
        "id": "_XRwbN9h4t6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block of code defines a property _llm_type for the TogetherLLM class. The @property decorator allows this method to be accessed like an attribute, meaning you can call instance._llm_type instead of instance._llm_type(). This is a cleaner and more intuitive way to identify the type of LLM being used within the TogetherAI-LangChain integration. Its purpose can be summarized as:\n",
        "\n",
        "Internal Use: The _llm_type property can be used for type checking, logging, or conditionally executing different logic based on the type of LLM. This flexibility is critical for extending or customizing behavior in more complex applications.\n",
        "Improved Readability: By using @property, the code becomes cleaner and easier to maintain. The LLM type can be retrieved without requiring method calls, simplifying the process of interacting with the LLM.\n",
        "2. Using the __call__ Method\n",
        "The __call__ method allows an instance of the TogetherLLM class to act like a function. By defining this method, the class instances become callable, making the code more intuitive and readable:"
      ],
      "metadata": {
        "id": "98R3N0lF48LU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def __call__(self, prompt: str) -> str:\n",
        "    # Check cache and run the LLM on the given prompt and input\n",
        "    return self._run_llm(prompt)\n"
      ],
      "metadata": {
        "id": "HlWfppms439o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Callable Object: Instead of invoking a method like instance.call(prompt), users can simply use instance(prompt), making the API more concise and aligned with functional programming practices.\n",
        "Cache Checking and Execution: The __call__ method is designed to first check if the response to a particular prompt is cached, thus avoiding redundant LLM executions. If not cached, the prompt is passed to the TogetherLLM for processing.\n",
        "Integration with Python Libraries: The __call__ method allows the TogetherLLM to be used seamlessly with Python features that expect callable objects, such as higher-order functions, improving the overall flexibility of the system.\n",
        "In summary, this code integrates TogetherAI with LangChain by simplifying interactions with the LLM. The _llm_type property is used to identify the LLM type, and the __call__ method makes the LLM callable, providing a clean and efficient API that is both intuitive and powerful for developers working on large-scale LLM applications."
      ],
      "metadata": {
        "id": "3rkcpGbm5Epm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8c75twXk5FNg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}